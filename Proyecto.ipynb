{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Computational Infrastructure Simulation Project\n",
    "## Cuantitative Methods, Spring 2017\n",
    "## Tec de Monterrey, campus Monterrey\n",
    "\n",
    "Marco A. Peyrot (A00815262)\n",
    "\n",
    "Oliver D. Mendoza (A00513632)\n",
    "\n",
    "Juan Carlos GuzmÃ¡n (A01175826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the libraries required to anaylze the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm, exponweib, lognorm\n",
    "\n",
    "# dynamic plotting information\n",
    "py.plotly.tools.set_credentials_file(username='mpeyrotc', api_key='pNScWhvJN9woL3frF4Vd')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A) Statistical Analysis\n",
    "\n",
    "## Principal Components Selection (Cause-Effect)\n",
    "\n",
    "Starting from the data provided by the course staff about the performance measures for the servers we transferred the data into python through the Pandas library. We take care of cleaning up the data by eliminating null rows and\n",
    "and transforming the time column to a more friendly numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open excel file\n",
    "R3P = pd.ExcelFile(\"R3P Q2 Mayo (ADV).xlsx\")\n",
    "# Open a specific spreadsheet\n",
    "R3P_1 = R3P.parse(\"R3P Q2 Mayo\")\n",
    "\n",
    "R3P_1 = R3P_1.ix[4:,3:31]\n",
    "\n",
    "R3P_1.columns=['Weekday', 'Time', 'Act. WPs', 'Dia.WPs', 'RFC WPs', 'CPU Usr',\n",
    "            'CPU Sys', 'CPU Idle', 'Paging in', 'Paging out', 'Free Mem.', \n",
    "            'EM alloc.', 'EM attach.', 'Em global', 'Heap Memor', 'Private Modes',\n",
    "            'Paging Mem', 'Roll Mem', 'Logins', 'Sessions', '# Pasos Dialogo',\n",
    "            'Resp. Time (Total)', 'CPU (Total)', 'CPU (Prom)', 'BD (Total)', \n",
    "            'BD (Prom)', 'Response Time (Prom)', 'Label']\n",
    "\n",
    "# Clean and reformat data\n",
    "R3P_1.dropna(how=\"all\", inplace=True)\n",
    "R3P_1['Time'] = R3P_1['Time'].astype(str).apply(lambda x: str(x[0]) + str(x[1]))\n",
    "\n",
    "X = R3P_1.ix[:,0:-1]\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once we have loaded the data, we proceed to do PCA (Principal component analysis). We get the contribution that each component provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the probability contribution of all eigen vectors gives 1.\n",
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "    \n",
    "print('Everything ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make cumulative line to visualiza better their contributions\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    plt.bar(range(27), var_exp, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.step(range(27), cum_var_exp, where='mid',\n",
    "             label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis\n",
    "\n",
    "With the results provided by the previous analysis we determined that the first 3 components contribute the most to the actual values. We use biplots to determine correlations between the variables we have. In order to get the PCA analysis for the bibplots we used XLSTAT and loaded the results for F1 and F2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PCAs = pd.ExcelFile(\"PCAs.xlsx\")\n",
    "F1F2 = PCAs.parse(\"F1F2\")\n",
    "\n",
    "F1F2_X = F1F2.ix[104:130,1:28]\n",
    "\n",
    "columns = []\n",
    "for i in xrange(0,27):\n",
    "    columns.append(\"F\" + str(i + 1))\n",
    "\n",
    "F1F2_X.columns=columns\n",
    "\n",
    "F1F2_y = F1F2.ix[104:130,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components F1 and F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    plt.scatter(F1F2_X.T.values[0], F1F2_X.T.values[1], c=\"Orange\")\n",
    "    plt.ylabel('F2 (13.40%)')\n",
    "    plt.xlabel('F1 (35.83%)')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1,1])\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    discriminators = [26, 17, 11, 20, 13, 18, 19, 16, 5]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=len(discriminators), desat=.5)\n",
    "    \n",
    "    for x,y,closest,label in zip(F1F2_X.T.values[0], F1F2_X.T.values[1], range(0,27), F1F2_y.T.values[0]):\n",
    "        plt.plot([0,x], [0, y], '-', linewidth=0.3, c=\"Green\")\n",
    "        if closest not in  discriminators:\n",
    "            plt.annotate(label, xy=(x, y), xytext=(x, y + 0.01))\n",
    "\n",
    "    legend = []\n",
    "    for index, color in zip(discriminators, colors.as_hex()):\n",
    "        plt.scatter(F1F2_X.T.values[0][index], F1F2_X.T.values[1][index], c=color)\n",
    "        legend.append(mpatches.Patch(color=color, label=F1F2_y.T.values[0][index]))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handles=legend)\n",
    "    \n",
    "    plt.plot([-1, 1], [0, 0], '--', linewidth=1, c=\"Black\")\n",
    "    plt.plot([0, 0], [-1, 1], '--', linewidth=1, c=\"Black\")\n",
    "    circle = plt.Circle((0, 0), 1, color='Black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components F1 and F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    \n",
    "    plt.scatter(F1F2_X.T.values[0], F1F2_X.T.values[2], c=\"Orange\")\n",
    "    plt.ylabel('F3 (6.62%)')\n",
    "    plt.xlabel('F1 (35.83%)')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1,1])\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    discriminators = [9, 3, 26, 17, 11, 18, 13, 19, 16]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=len(discriminators), desat=.5)\n",
    "    \n",
    "    for x,y,closest,label in zip(F1F2_X.T.values[0], F1F2_X.T.values[2], range(0,27), F1F2_y.T.values[0]):\n",
    "        plt.plot([0,x], [0, y], '-', linewidth=0.3, c=\"Green\")\n",
    "        if closest not in  discriminators:\n",
    "            plt.annotate(label, xy=(x, y), xytext=(x, y + 0.01))\n",
    "            \n",
    "    legend = []\n",
    "    for index, color in zip(discriminators, colors.as_hex()):\n",
    "        plt.scatter(F1F2_X.T.values[0][index], F1F2_X.T.values[2][index], c=color)\n",
    "        legend.append(mpatches.Patch(color=color, label=F1F2_y.T.values[0][index]))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handles=legend)\n",
    "    \n",
    "    plt.plot([-1, 1], [0, 0], '--', linewidth=1, c=\"Black\")\n",
    "    plt.plot([0, 0], [-1, 1], '--', linewidth=1, c=\"Black\")\n",
    "    circle = plt.Circle((0, 0), 1, color='Black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components F2 and F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    plt.scatter(F1F2_X.T.values[1], F1F2_X.T.values[2], c=\"Orange\")\n",
    "    plt.ylabel('F3 (6.62%)')\n",
    "    plt.xlabel('F2 (13.40%)')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1,1])\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    discriminators = [11, 19, 18, 17, 16, 13, 9]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=len(discriminators), desat=.5)\n",
    "    \n",
    "    for x,y,closest,label in zip(F1F2_X.T.values[1], F1F2_X.T.values[2], range(0,27), F1F2_y.T.values[0]):\n",
    "        plt.plot([0,x], [0, y], '-', linewidth=0.3, c=\"Green\")\n",
    "        if closest not in  discriminators:\n",
    "            plt.annotate(label, xy=(x, y), xytext=(x, y + 0.01))\n",
    "            \n",
    "    legend = []\n",
    "    for index, color in zip(discriminators, colors.as_hex()):\n",
    "        plt.scatter(F1F2_X.T.values[1][index], F1F2_X.T.values[2][index], c=color)\n",
    "        legend.append(mpatches.Patch(color=color, label=F1F2_y.T.values[0][index]))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handles=legend)\n",
    "    \n",
    "    plt.plot([-1, 1], [0, 0], '--', linewidth=1, c=\"Black\")\n",
    "    plt.plot([0, 0], [-1, 1], '--', linewidth=1, c=\"Black\")\n",
    "    circle = plt.Circle((0, 0), 1, color='Black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical variables selection and dimension reduction\n",
    "\n",
    "Based on the obtained biplots we determined that the following variables are correlated to response time (average) our objective. According to the principal components F1 and F2, relevant variables are:\n",
    "\n",
    "1. BD (Prom)\n",
    "2. CPU(Prom)\n",
    "3. Private modes\n",
    "4. RFC WPs\n",
    "5. Time\n",
    "\n",
    "In the same way, using components F1 and F3 the following variables are also related to response time (average):\n",
    "\n",
    "6. CPU Idle\n",
    "7. CPU Usr\n",
    "\n",
    "Next we show the behavior graphs for each one of these variables in relation to response time. The Orange graphs represent the ones with a visible relation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    plt.subplot(4, 3, 1)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"BD (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('DB (Average) [ms]')\n",
    "    \n",
    "    plt.subplot(4, 3, 2)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    \n",
    "    plt.subplot(4, 3, 3)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,9])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"Private Modes\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('Private Modes [Number]')\n",
    "    \n",
    "    plt.subplot(4, 3, 4)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,40])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"RFC WPs\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('RFC WPs [Number]')\n",
    "    \n",
    "    plt.subplot(4, 3, 5)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,23])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"Time\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('Time [hours]')\n",
    "    \n",
    "    plt.subplot(4, 3, 6)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,100])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU Idle\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU Idle [%]')\n",
    "    \n",
    "    plt.subplot(4, 3, 7)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU Usr\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU Usr [%]')\n",
    "    \n",
    "    response_time = R3P_1[\"Response Time (Prom)\"].apply(math.log10)\n",
    "    cpu_prom = R3P_1[\"CPU (Prom)\"].apply(math.log10)\n",
    "    \n",
    "    plt.subplot(4, 3, 8)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,5])\n",
    "    axes.set_ylim([0,5])\n",
    "    plt.scatter(response_time.T, cpu_prom.T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one was done in a log scale to produce a more precise relation between them. These graphs clearly show a relation between DB (Prom) y CPU(Prom) as potential variables to improve the system performance. We propose mathematical functions which are representative of the system behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    x = R3P_1.loc[:,\"Response Time (Prom)\"][:, np.newaxis]\n",
    "    y = R3P_1.loc[:,\"BD (Prom)\"]\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"BD (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('DB (Average) [ms]')\n",
    "    plt.plot(x, regr.predict(x), color='blue', linewidth=1)\n",
    "    \n",
    "    plt.text(21500, 33000, r'Equation: $y={coef}x+{intercept}$'.format(coef=\"{0:.2f}\".format(regr.coef_[0]), \n",
    "            intercept=\"{0:.2f}\".format(regr.intercept_)), fontsize=12, style='italic', \n",
    "             bbox={'facecolor':'gray', 'alpha':0.1, 'pad':2})\n",
    "\n",
    "    x = R3P_1.loc[:,\"Response Time (Prom)\"][:, np.newaxis]\n",
    "    y = R3P_1.loc[:,\"CPU (Prom)\"]\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    plt.plot(x, regr.predict(x), color='blue', linewidth=1)\n",
    "    plt.text(21300, 33000, r'Equation: $y={coef}x+{intercept}$'.format(coef=\"{0:.2f}\".format(regr.coef_[0]), \n",
    "                                                                       intercept=\"{0:.2f}\".format(regr.intercept_)),\n",
    "             fontsize=12, style='italic', bbox={'facecolor':'gray', 'alpha':0.1, 'pad':2})\n",
    "    plt.title(\"Response Time vs. CPU (Average)\")\n",
    "    \n",
    "    x = response_time[:, np.newaxis]\n",
    "    y = cpu_prom\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,5])\n",
    "    axes.set_ylim([0,5])\n",
    "    plt.scatter(response_time.T, cpu_prom.T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    plt.plot(x, regr.predict(x), color='blue', linewidth=1)\n",
    "    plt.text(1.5, 4.7, r'Equation: $y={coef}x+{intercept}$'.format(coef=\"{0:.2f}\".format(regr.coef_[0]), \n",
    "                                                                       intercept=\"{0:.2f}\".format(regr.intercept_)),\n",
    "             fontsize=12, style='italic', bbox={'facecolor':'gray', 'alpha':0.1, 'pad':2})\n",
    "    plt.title(\"Response Time vs. CPU (Average) in log scale\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly for this data the outliers drastically affect the fitting of a general equation. However this is not the case for the CPU average. However, to do a more thorough analysis, we fitted several different trendlines to the data, and the best results yielded were the following:\n",
    "\n",
    "![CPU](CPU trendline.png)\n",
    "![BD](DB trendline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistic analysis\n",
    "\n",
    "Because we found not one, but two critical variables (BD and CPU), we will analyze both, to see which one is more reliable. We will measure its mean, varianza, standard deviation, symetry and kurtosis for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"CPU (Prom):\\n\")\n",
    "print \"Mean:\\t\\t\\t\", R3P_1[\"CPU (Prom)\"].mean()\n",
    "print \"Variance:\\t\\t\", R3P_1[\"CPU (Prom)\"].var()\n",
    "print \"Standard deviation:\\t\", R3P_1[\"CPU (Prom)\"].std()\n",
    "print \"Symetry:\\t\\t\", R3P_1[\"CPU (Prom)\"].skew()\n",
    "print \"Kurtosis:\\t\\t\", R3P_1[\"CPU (Prom)\"].kurtosis()\n",
    "print\n",
    "print(\"BD (Prom):\\n\")\n",
    "print \"Mean:\\t\\t\\t\", R3P_1[\"BD (Prom)\"].mean()\n",
    "print \"Variance:\\t\\t\", R3P_1[\"BD (Prom)\"].var()\n",
    "print \"Standard deviation:\\t\", R3P_1[\"BD (Prom)\"].std()\n",
    "print \"Symetry:\\t\\t\", R3P_1[\"BD (Prom)\"].skew()\n",
    "print \"Kurtosis:\\t\\t\", R3P_1[\"BD (Prom)\"].kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be easily seen that both variables have a heavy tail, since kurtosis is greater than 3. You can see the histogram of the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as p\n",
    "\n",
    "data=R3P_1[\"CPU (Prom)\"]\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"CPU (Prom)\")\n",
    "\n",
    "data=R3P_1[\"BD (Prom)\"]\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"BD (Prom)\")\n",
    "\n",
    "p.legend(loc='upper right')\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be observed that both variables have a similar shape to that of an exponential distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Determining the normality of the data\n",
    "Below, a comparison of both variables that have been discused previously against the exponential distribution will be shown. Originally, our intention was to compare it to the normal distribution, but since it was observed that they had similar behavior to exponential distributions, it was decided to compare against that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "from scipy.stats import norm\n",
    "\n",
    "expectedValues = pd.Series(range(1 ,R3P_1[\"CPU (Prom)\"].count() + 1)).apply(lambda x: (x - 0.5) / R3P_1[\"CPU (Prom)\"].count()).apply(lambda x: expon.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"CPU (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='CPU (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for exponential quartile.'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison against exponential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expectedValues = pd.Series(range(1 ,R3P_1[\"BD (Prom)\"].count() + 1)).apply(lambda x: (x - 0.5) / R3P_1[\"BD (Prom)\"].count()).apply(lambda x: expon.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"BD (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='BD (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for exponential quartile.'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison against exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be observed that both plots have a shape that could, in some way, describe a straight line, so it can be said that, in fact,they have a distribution similar to an exponential. However, its important to note that the variable BD forms something similar to an inverted 'S', which tells us that it has a heavy tail distribution. If we wanted to comparte the variables with the normal distribution, this is what we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "from scipy.stats import norm\n",
    "\n",
    "expectedValues = pd.Series(range(1 ,R3P_1[\"CPU (Prom)\"].count() + 1))\n",
    "expectedValues = expectedValues.apply(lambda x: (x - 0.5) / R3P_1[\"CPU (Prom)\"].count())\n",
    "expectedValues = expectedValues.apply(lambda x: norm.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"CPU (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='CPU (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for normal quartile'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison with normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expectedValues = pd.Series(range(1 ,R3P_1[\"BD (Prom)\"].count() + 1))\n",
    "expectedValues = expectedValues.apply(lambda x: (x - 0.5) / R3P_1[\"BD (Prom)\"].count())\n",
    "expectedValues = expectedValues.apply(lambda x: norm.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"BD (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='BD (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for normal quartile'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison with normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As it can be seen, instead of looking like straight lines, they seem curved, which tells us that if we compared the with normal distributions, they would be asymetrical, which suggests that the data is not \"normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main variables Analysis\n",
    "\n",
    "The CPU (Average) and Database (average) where the main vaiables that affect response time (seen above). Thankfully we have the traces for that data for a significant period of time. We performed, as last time, PCA analysis over the execution traces for both BD and CPU and proceeded to do the analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## CPU ####################\n",
    "# Obtiene el dataframe de excel\n",
    "CPU_File = pd.ExcelFile(\"CPU Prom Trace.xlsx\")\n",
    "CPU_FactorScores = CPU_File.parse(\"Factor Scores\")\n",
    "\n",
    "# Tabla de Factor Scores\n",
    "Factor_Scores = CPU_FactorScores.ix[:8195,:]\n",
    "\n",
    "# Set X\n",
    "X = Factor_Scores.loc[:,'F1'].values\n",
    "X = np.array(X)\n",
    "\n",
    "CPU_EigenValues = CPU_File.parse(\"Eigen values\")\n",
    "LambdaCPU = CPU_EigenValues.iloc[0]['F1']\n",
    "\n",
    "# Medias y desviaciones estandar Originales\n",
    "MediaOriginalCPU = 2323.994\n",
    "desvEstandarCPU = 552.579\n",
    "\n",
    "# Obtiene las Y\n",
    "Y_CPU = (X * desvEstandarCPU) / math.sqrt(LambdaCPU)\n",
    "\n",
    "for x in range(0, np.size(Y_CPU)):\n",
    "    Y_CPU[x] = Y_CPU[x] + MediaOriginalCPU\n",
    "\n",
    "# Saca la media\n",
    "MediaCPU = sum(Y_CPU)/np.size(Y_CPU)\n",
    "\n",
    "# Elimina los negativos\n",
    "Y_CPU = Y_CPU[Y_CPU >= 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## BD ###################\n",
    "# Obtiene el dataframe de excel\n",
    "BD_File = pd.ExcelFile(\"DB Prom Trace.xlsx\")\n",
    "BD_FactorScores = BD_File.parse(\"Factor Scores\")\n",
    "\n",
    "# Tabla de Factor Scores\n",
    "Factor_Scores = BD_FactorScores.ix[:8195,:]\n",
    "\n",
    "# Set X\n",
    "X = Factor_Scores.loc[:,'F1'].values\n",
    "X = np.array(X)\n",
    "\n",
    "BD_EigenValues = BD_File.parse(\"Eigen values\")\n",
    "LambdaBD = BD_EigenValues.iloc[0]['F1']\n",
    "\n",
    "# Medias y desviaciones estandar Originales\n",
    "MediaOriginalBD = 5038.738\n",
    "desvEstandarBD = 1771.288\n",
    "\n",
    "# Obtiene las Y\n",
    "Y_BD = (X * desvEstandarBD) / math.sqrt(LambdaBD)\n",
    "\n",
    "for x in range(0, np.size(Y_BD)):\n",
    "    Y_BD[x] = Y_BD[x] + MediaOriginalBD\n",
    "\n",
    "# Saca la media\n",
    "MediaBD = sum(Y_BD)/np.size(Y_BD)\n",
    "\n",
    "# Elimina los negativos\n",
    "Y_BD = Y_BD[Y_BD >= 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the CT (Characteristic trace) for CPU and BD in Y_CPU and Y_BD respectively, we can make an statistical analysis of them. This was done the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as p\n",
    "\n",
    "data=Y_CPU\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"Y CPU\")\n",
    "\n",
    "data=Y_BD\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"Y BD\")\n",
    "\n",
    "p.legend(loc='upper right')\n",
    "\n",
    "p.show()\n",
    "\n",
    "Y_CPU2 = pd.Series(Y_CPU)\n",
    "\n",
    "print(\"Y CPU:\\n\")\n",
    "print \"Mean:\\t\\t\\t\", Y_CPU2.mean()\n",
    "print \"Variance:\\t\\t\", Y_CPU2.var()\n",
    "print \"Standard Deviation:\\t\", Y_CPU2.std()\n",
    "print \"Symmetry:\\t\\t\", Y_CPU2.skew()\n",
    "print \"Kurtosis:\\t\\t\", Y_CPU2.kurtosis()\n",
    "\n",
    "Y_BD2 = pd.Series(Y_BD)\n",
    "\n",
    "print\n",
    "print(\"Y BD:\\n\")\n",
    "print \"Mean:\\t\\t\\t\", Y_BD2.mean()\n",
    "print \"Variance:\\t\\t\", Y_BD2.var()\n",
    "print \"Standard Deviation:\\t\", Y_BD2.std()\n",
    "print \"Symmetry:\\t\\t\", Y_BD2.skew()\n",
    "print \"Kurtosis:\\t\\t\", Y_BD2.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can conclude that, since kurtosis is less than 3 in both cases, they are light tailed, and we can see that they are both failry symetrical; not exactly, but enough. Another interesting fact is that while the standard deviation is roughly 23% of its mean in CPU and almost 35% in the case of DB. This shows us that the CPU variable is more consistent the than DB, which could be a relevant piece of indormation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probability distribution adjustment\n",
    "\n",
    "Due to the dificulty and lack of experience in using probability distributions in python, we decided to use XLSTAT again to fit the CPU Y and BD Y vectors.\n",
    "\n",
    "In this section we put the values and graph we got for the following distributions:\n",
    "\n",
    "* Exponencial\n",
    "* Gamma\n",
    "* Weibull 2\n",
    "* Weibull 3\n",
    "* Lognormal\n",
    "* Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass CPU Y and BD Y to csv files to be read by excel\n",
    "Y_CPU.tofile('Y_CPU.csv',sep=',',format='%10.5f')\n",
    "Y_BD.tofile('Y_BD.csv',sep=',',format='%10.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For CPU\n",
    "## Exponential distribution\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|2324.185\t|2324.934|\n",
    "|Variance\t|304725.697\t|304823.833|\n",
    "|Skewness (Pearson)|0.710|\t0.475|\n",
    "|Kurtosis (Pearson)\t|1.613\t|0.338|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.028\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Gamma 2 fit distribution](gamma2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353\t|5036.048|\n",
    "|Variance\t|3135127.217\t|3133071.032|\n",
    "|Skewness (Pearson)|\t0.726|\t0.703|\n",
    "|Kurtosis (Pearson)|\t0.979|\t0.741|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.008\t\n",
    "p-value:\t0.594\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Gamma 2 fit distribution](gamma2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weibull 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|2324.185|\t2310.376|\n",
    "|Variance|\t304725.697|\t379195.850|\n",
    "|Skewness (Pearson)|\t0.710|\t-0.132|\n",
    "|Kurtosis (Pearson)|\t1.613|\t-0.226|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "D: 0.069\t\n",
    "p-value: < 0.0001\t\n",
    "alpha: 0.05\n",
    "\n",
    "![Weibull 2 fit distribution](weibull2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weibull 3\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "| Mean\t|2324.185|\t2324.185|\n",
    "| Variance|\t304725.697|\t295057.687|\n",
    "| Skewness (Pearson)|\t0.710|\t-0.082|\n",
    "| Kurtosis (Pearson)|\t1.613|\t-0.255|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.059\t\n",
    "p-value: < 0.0001\t\n",
    "alpha:\t0.05\t\n",
    "\n",
    "![Weibull 3 fit distribution](weibull3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lognormal\n",
    "\n",
    "| Statistic |\tData|\tParameters|\n",
    "| ------ |\n",
    "|Mean|\t2324.185|\t2325.697|\n",
    "|Variance|\t304725.697|\t320582.598|\n",
    "|Skewness (Pearson)\t|0.710 |\t0.745|\n",
    "|Kurtosis (Pearson)\t|1.613 |\t1.002|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.036\n",
    "\n",
    "p-value: < 0.0001\n",
    "\n",
    "alpha: 0.05\n",
    "\n",
    "![Lognormal fit distribution](lognormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pareto\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|2324.185\t|573728884701.262|\n",
    "|Variance|\t304725.697\t|86292.727|\n",
    "|Skewness (Pearson)\t|0.710|\t2.000|\n",
    "|Kurtosis (Pearson)\t|1.613|\t6.000|\n",
    "\n",
    "Kolmogorov-Smirnov test:\t\n",
    "D:\t0.000\t\n",
    "p-value:\t1.000\t\n",
    "alpha:\t0.05\n",
    "\n",
    "This is an interesting case because the distribution is way off. So no line appears.\n",
    "\n",
    "![Pareto fit distribution](pareto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLSTAT provided us we a very useful value for such fittings, the Kolmogorov-Smirnov test. After some investigation we discovered that this value is used to decide if a sample comes from a population with a specific distribution. This is true for continuous distributions, which we have!\n",
    "\n",
    "For CPU the 3 best fittings where the Exponential, the Gamma 2 and the lognormal with d values 0.028, 0.008, and 0.036 respectively. We made the QQ plots for further analysis for those distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon, lognorm, gamma, weibull_min\n",
    "import statsmodels.api as sm\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_CPU, expon, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Exponential QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_CPU, gamma, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Gamma 2 QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_CPU, lognorm, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Log-normal QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,2])\n",
    "    axes.set_ylim([0,2])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is important for the reader to note that the Pareto distribution gives the D with the lowest value. However it does not correlate visually and we belive that this comes from en error in the tools used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Database\n",
    "## Log normal\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353|\t5057.947|\n",
    "|Variance\t|3135127.217|\t3687505.544|\n",
    "|Skewness (Pearson)\t|0.726|\t1.194|\n",
    "|Kurtosis (Pearson)\t|0.979|\t2.636|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.025\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Log normal fit distribution](lognormaldb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibull 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353\t|5014.704|\n",
    "|Variance|\t3135127.217\t|3349855.894|\n",
    "|Skewness (Pearson)|\t0.726|\t0.173|\n",
    "|Kurtosis (Pearson)\t|0.979|\t-0.269|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.044\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Weibull 2 fit distribution](weibull2db.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibull 3\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean|\t5039.353|\t5039.353|\n",
    "|Variance|\t3135127.217\t|2435772.411|\n",
    "|Skewness (Pearson)|\t0.726\t|0.074|\n",
    "|Kurtosis (Pearson)|\t0.979\t|-0.289|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D:\t0.054\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Weibull 3 fit distribution](weibull3db.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353\t|579254965061.702|\n",
    "|Variance\t|3135127.217\t|156064.120|\n",
    "|Skewness (Pearson)|\t0.726|\t2.000|\n",
    "|Kurtosis (Pearson)\t|0.979\t|6.000|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.000\t\n",
    "p-value: \t1.000\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Pareto fit distribution](paretodb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5117.023\t|5117.023|\n",
    "|Variance\t|3219173.378|\t0.000|\n",
    "|Skewness (Pearson)\t|0.756\t|2.000|\n",
    "|Kurtosis (Pearson)\t|1.019|\t6.000|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D:\t0.353\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Exponential fit distribution](exponentialdb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353|\t5036.048|\n",
    "|Variance|\t3135127.217\t|3133071.032|\n",
    "|Skewness (Pearson)|\t0.726|\t0.703|\n",
    "|Kurtosis (Pearson)\t|0.979|\t0.741|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D:\t0.008\t\n",
    "p-value:\t0.594\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Gamma 2 fit distribution](gamma2db.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Similarly, we can see that for the database the lognormal, weibull 2 y gamma 2 distributions offer the best fit according to the Kolmogorov-Smirnov test. The D values that correspond to each distribution are: 0.025, 0.044, 0.008.\n",
    "\n",
    "Again we plot for these distributions with QQPlots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_BD, lognorm, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Lognormal QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,10000])\n",
    "    axes.set_ylim([0,10000])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We found an outlier that greatly affects the distribution. Making this distribution infeasible for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_BD, weibull_min, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Weibull 2 QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,10000])\n",
    "    axes.set_ylim([0,10000])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again, the outlliers in the database trace make this distribution not suited for our use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_BD, gamma, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Gamma 2 QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These  results gave us confidence that our system behaves like a gamma probability distribution for both CPU and DB. Needless to say, the Pareto distribution failed returned dubious results. We conclude that we should treat these variables as gamma distribuited and base our future analysis in part B under the assumption that this is their behavior.\n",
    "\n",
    "# Part B) Simulation\n",
    "\n",
    "Continuing with our analysis, we wish to simulate the operation of the system and determine the load it can take with a 95% confidence level.\n",
    "\n",
    "According to the data taken by a survey made the past 14th of november the average arrival rate was $1/\\lambda=19212$\n",
    "requests per hour (5.3366 requests per second). The system is composed of 6 processors with 8 cores each.\n",
    "\n",
    "The queueing system uses a geometric distribution of p=0.5 to simulate the passing of jobs through the system.\n",
    "\n",
    "\n",
    "## Queue system\n",
    "\n",
    "For the simulation we are taking into account that the system has a sigle queue with a message feedback loop. The system is considered to have unlimited memory.\n",
    "\n",
    "The simulation will make use of the following M/M/1 program and the feedback loop will be simulated by using the formula shown below.\n",
    "\n",
    "<center>$serviceTime = Random(1,5)X_{CPU} + Random(1,4)BD + additionalData$</center>\n",
    "\n",
    "where additional data is:\n",
    "\n",
    "* Wait Time: 30 ms\n",
    "* Roll-In Time: 30 ms\n",
    "* Load Time: 300 ms\n",
    "* Processing Time: 1200-1300 ms\n",
    "* Database Time: 1200-1300 ms\n",
    "* Roll-Out Time: 30 ms\n",
    "\n",
    "In this case, we will be using the average for Database time that we had calculated in previous phases, which is 5.039. The result of the simulation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "\n",
    "class Processor:\n",
    "    \"\"\"\n",
    "    finish_time: the time in which the processor will finish handling its current job.\n",
    "    is_free: determines if the processor is free to take on a new job.\n",
    "    \"\"\"\n",
    "    finish_time = 0\n",
    "    is_free = True\n",
    "    \n",
    "    def __init__(self, finish_time, is_free):\n",
    "        self.finish_time = finish_time\n",
    "        self.is_free = is_free\n",
    "\n",
    "def next_service_time(X):\n",
    "    amount = np.random.geometric(p = 0.5)\n",
    "    if amount > 4:\n",
    "        amount = 4\n",
    "    # 5.039\n",
    "    result = (amount * X) + ((amount+1) * X)\n",
    "    return result + 0.03 + 0.03 + 0.3 + 0.03\n",
    "\n",
    "db_time = 1.5\n",
    "residence_time= 6\n",
    "\n",
    "residence_times = []\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_free_processor(processors):\n",
    "    \"\"\"\n",
    "    gets a random free processor from the available ones\n",
    "\n",
    "    processors: all the processors in the system.\n",
    "    returns: the Processor that was selected and is curently free to take a job.\n",
    "    \"\"\"\n",
    "    free_processors = []\n",
    "    \n",
    "    for processor in processors:\n",
    "        if processor.is_free:\n",
    "            free_processors.append(processor)\n",
    "            \n",
    "    return free_processors[random.randint(0, len(free_processors) - 1)]\n",
    "\n",
    "def at_least_one_free(processors):\n",
    "    \"\"\"\n",
    "    determines if there is at least one free processor\n",
    "    \n",
    "    processors: processors all the processors present in the system.\n",
    "    returns: true if there is at least one free processor and false otherwise.\n",
    "    \"\"\"\n",
    "    for processor in processors:\n",
    "        if processor.is_free:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_candidate_processor(processors, arrival_time):\n",
    "    \"\"\"\n",
    "    finds the processor which will finish first or one of the free processors (if any)\n",
    "    \n",
    "    processors:  the processors present in the system\n",
    "    arrival_time: the next event arrival time\n",
    "    returns: the selected processor to take the new event (arrival or departure)\n",
    "    \"\"\"\n",
    "    result = processors[0]\n",
    "    \n",
    "    for processor in processors:\n",
    "        if processor.finish_time < result.finish_time:\n",
    "            result = processor\n",
    "            \n",
    "    if result.finish_time > arrival_time and at_least_one_free(processors):\n",
    "        return find_free_processor(processors)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global variables for the simulation\n",
    "processors = []\n",
    "num_processors = 48\n",
    "total_observation_time = 5000 # cambiar\n",
    "current_time = 0\n",
    "arrival_time = 0\n",
    "num_jobs = 0\n",
    "max_jobs = 999999 # cambiar\n",
    "interarrival_time = 1 / 5.3366 # cambiar (JUGAR)\n",
    "last_event = 0\n",
    "time_diff = 0\n",
    "S = 0\n",
    "expected_service_time = 2.6 # cambiar\n",
    "busy_time = 0\n",
    "completed_jobs = 0\n",
    "num_rejected_jobs = 0\n",
    "total_jobs = 0\n",
    "\n",
    "# initialize processors to free\n",
    "for i in xrange(0,num_processors):\n",
    "    processors.append(Processor(999999, True))\n",
    "    \n",
    "while current_time < total_observation_time:\n",
    "    processor = find_candidate_processor(processors, arrival_time)\n",
    "    \n",
    "    # it can be free or occupied\n",
    "    if arrival_time <= processor.finish_time:\n",
    "        if num_jobs < max_jobs:\n",
    "            current_time = arrival_time\n",
    "            num_jobs += 1\n",
    "            time_diff = current_time - last_event\n",
    "            S += (num_jobs - 1) * time_diff\n",
    "            last_event = current_time\n",
    "            \n",
    "            arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "            \n",
    "            if num_jobs <= len(processors):\n",
    "                service_time = next_service_time(5.039)\n",
    "                processor.finish_time = current_time + service_time\n",
    "                busy_time += min(service_time, total_observation_time - current_time)\n",
    "                processor.is_free = False\n",
    "        else:\n",
    "            arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "            num_rejected_jobs += 1\n",
    "    else:\n",
    "        # there has been a departure from the system\n",
    "        current_time = processor.finish_time\n",
    "        num_jobs -= 1\n",
    "        completed_jobs += 1\n",
    "        time_diff = current_time - last_event\n",
    "        S += (num_jobs + 1) * time_diff\n",
    "        last_event = current_time\n",
    "\n",
    "        if num_jobs >= len(processors):\n",
    "            service_time = next_service_time(5.039)\n",
    "            processor.finish_time = current_time + service_time\n",
    "            busy_time += min(service_time, total_observation_time - current_time)\n",
    "            processor.is_free = False\n",
    "        else:\n",
    "            processor.is_free = True\n",
    "            processor.finish_time = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"N: {0}\".format(S/current_time)\n",
    "print \"U: {0}\".format(busy_time/(current_time * len(processors)))\n",
    "print \"R: {0}\".format(S/completed_jobs)\n",
    "print \"X: {0}\".format(completed_jobs/current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that the values are very high. If we had used the average that came with the definition of the problem (1.25), the results would be the following:\n",
    "\n",
    "N: 33.9622966712\n",
    "\n",
    "U: 0.707036511606\n",
    "\n",
    "R: 6.32857482319\n",
    "\n",
    "X: 5.36649998145\n",
    "\n",
    "This makes sense, since the average time for DB is ambout 4 times smaller that the one we calculated. Now, we will evaluate, through simulations, how much time reduction is needed to make the system faster. Since N in the simulations with the data provided to us (not the one we calculated, which was way bigger) is less than 48, which is the amount of processors, it can be assumed that there are no jobs on queue. Thus, the solution to making the system faster is not to increase processors or interarrival time, but to decrease the service time, by decreasing either CPU or DB speed. We will approach this by multiplying the average time in DB and CPU by 0.95 on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_service_time(X):\n",
    "    amount = np.random.geometric(p = 0.5)\n",
    "    if amount > 4:\n",
    "        amount = 4\n",
    "    # 5.039\n",
    "    result = (amount * X) + ((amount+1) * X)\n",
    "    return result + 0.03 + 0.03 + 0.3 + 0.03\n",
    "\n",
    "db_time = 1.5\n",
    "residence_time= 6\n",
    "\n",
    "residence_times = []\n",
    "times = []\n",
    "\n",
    "while residence_time > 5:\n",
    "\n",
    "    # Global variables for the simulation\n",
    "    processors = []\n",
    "    num_processors = 48\n",
    "    total_observation_time = 5000 # cambiar\n",
    "    current_time = 0\n",
    "    arrival_time = 0\n",
    "    num_jobs = 0\n",
    "    max_jobs = 999999 # cambiar\n",
    "    last_event = 0\n",
    "    time_diff = 0\n",
    "    S = 0\n",
    "    expected_service_time = 2.6 # cambiar\n",
    "    busy_time = 0\n",
    "    completed_jobs = 0\n",
    "    num_rejected_jobs = 0\n",
    "    total_jobs = 0\n",
    "\n",
    "    # initialize processors to free\n",
    "    for i in xrange(0,num_processors):\n",
    "        processors.append(Processor(999999, True))\n",
    "\n",
    "    while current_time < total_observation_time:\n",
    "        processor = find_candidate_processor(processors, arrival_time)\n",
    "\n",
    "        # it can be free or occupied\n",
    "        if arrival_time <= processor.finish_time:\n",
    "            if num_jobs < max_jobs:\n",
    "                current_time = arrival_time\n",
    "                num_jobs += 1\n",
    "                time_diff = current_time - last_event\n",
    "                S += (num_jobs - 1) * time_diff\n",
    "                last_event = current_time\n",
    "\n",
    "                arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "\n",
    "                if num_jobs <= len(processors):\n",
    "                    service_time = next_service_time(db_time)\n",
    "                    processor.finish_time = current_time + service_time\n",
    "                    busy_time += min(service_time, total_observation_time - current_time)\n",
    "                    processor.is_free = False\n",
    "            else:\n",
    "                arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "                num_rejected_jobs += 1\n",
    "        else:\n",
    "            # there has been a departure from the system\n",
    "            current_time = processor.finish_time\n",
    "            num_jobs -= 1\n",
    "            completed_jobs += 1\n",
    "            time_diff = current_time - last_event\n",
    "            S += (num_jobs + 1) * time_diff\n",
    "            last_event = current_time\n",
    "\n",
    "            if num_jobs >= len(processors):\n",
    "                service_time = next_service_time(db_time)\n",
    "                processor.finish_time = current_time + service_time\n",
    "                busy_time += min(service_time, total_observation_time - current_time)\n",
    "                processor.is_free = False\n",
    "            else:\n",
    "                processor.is_free = True\n",
    "                processor.finish_time = 999999\n",
    "    residence_time = S/completed_jobs\n",
    "    print \"DB and CPU Time: {0}\".format(db_time)\n",
    "    print \"N: {0}\".format(S/current_time)\n",
    "    print \"U: {0}\".format(busy_time/(current_time * len(processors)))\n",
    "    print \"R: {0}\".format(S/completed_jobs)\n",
    "    print \"X: {0}\".format(completed_jobs/current_time)\n",
    "    print \"\"\n",
    "    \n",
    "    residence_times.insert(0, S/completed_jobs)\n",
    "    times.insert(0, db_time)\n",
    "    \n",
    "    db_time = db_time * 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can conclude from our results that the response time for the database and CPU systems should decrease considerably to achieve the desired system behavior. This can be seen in the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    \n",
    "    plt.scatter(times, residence_times, c=\"Orange\")\n",
    "    plt.ylabel('Response time')\n",
    "    plt.xlabel('DB and CPU time')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([2,0])\n",
    "    axes.set_ylim([0,8])\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly shows that better equipment for both database access and CPU processor speed is needed. However, we must take into account that the number of processors of the server system should remain as 48 to achive optimal response times. \n",
    "\n",
    "Therefore, the problem resides not in the interarrival time, but the service time; from the point in time we began to the optimal value obtained, the number in jobs in system never was above 48.\n",
    "\n",
    "Now we wish to visualize the behavior of the system while varying the interarrival time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_modifiers = np.arange(1,3,0.1)\n",
    "lambdas = []\n",
    "for value in lambda_modifiers:\n",
    "    lambdas.append(1 / (value * 5.3366))\n",
    "lambdas\n",
    "\n",
    "wait_time_arr = []\n",
    "\n",
    "# Global variables for the simulation\n",
    "def vary_interarrival_time(new_time):\n",
    "    db_time = 0.945374\n",
    "    residence_time= 6\n",
    "    processors = []\n",
    "    num_processors = 48\n",
    "    total_observation_time = 5000 # cambiar\n",
    "    current_time = 0\n",
    "    arrival_time = 0\n",
    "    num_jobs = 0\n",
    "    max_jobs = 999999 # cambiar\n",
    "    interarrival_time = new_time # cambiar (JUGAR)\n",
    "    last_event = 0\n",
    "    time_diff = 0\n",
    "    S = 0\n",
    "    expected_service_time = 2.6 # cambiar\n",
    "    busy_time = 0\n",
    "    completed_jobs = 0\n",
    "    num_rejected_jobs = 0\n",
    "    total_jobs = 0\n",
    "\n",
    "    # initialize processors to free\n",
    "    for i in xrange(0,num_processors):\n",
    "        processors.append(Processor(999999, True))\n",
    "    \n",
    "    while current_time < total_observation_time:\n",
    "        # Global variables for the simulation\n",
    "        processor = find_candidate_processor(processors, arrival_time)\n",
    "\n",
    "        # it can be free or occupied\n",
    "        if arrival_time <= processor.finish_time:\n",
    "            if num_jobs < max_jobs:\n",
    "                current_time = arrival_time\n",
    "                num_jobs += 1\n",
    "                time_diff = current_time - last_event\n",
    "                S += (num_jobs - 1) * time_diff\n",
    "                last_event = current_time\n",
    "\n",
    "                arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "\n",
    "                if num_jobs <= len(processors):\n",
    "                    service_time = next_service_time(db_time)\n",
    "                    processor.finish_time = current_time + service_time\n",
    "                    busy_time += min(service_time, total_observation_time - current_time)\n",
    "                    processor.is_free = False\n",
    "                    wait_time_arr.append(processor.finish_time - current_time)\n",
    "            else:\n",
    "                arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "                num_rejected_jobs += 1\n",
    "        else:\n",
    "            # there has been a departure from the system\n",
    "            current_time = processor.finish_time\n",
    "            num_jobs -= 1\n",
    "            completed_jobs += 1\n",
    "            time_diff = current_time - last_event\n",
    "            S += (num_jobs + 1) * time_diff\n",
    "            last_event = current_time\n",
    "\n",
    "            if num_jobs >= len(processors):\n",
    "                service_time = next_service_time(db_time)\n",
    "                processor.finish_time = current_time + service_time\n",
    "                busy_time += min(service_time, total_observation_time - current_time)\n",
    "                processor.is_free = False\n",
    "            else:\n",
    "                processor.is_free = True\n",
    "                processor.finish_time = 999999\n",
    "                \n",
    "    residence_time = S/completed_jobs\n",
    "    db_time = db_time * 0.95\n",
    "    \n",
    "    wait = np.array(wait_time_arr)\n",
    "    \n",
    "    successes = 0.0\n",
    "    for value in wait:\n",
    "        if(value <= 5):\n",
    "            successes += 1\n",
    "    \n",
    "    # N, U, R, X\n",
    "    return (S/current_time, busy_time/(current_time * len(processors)), \n",
    "            S/completed_jobs, completed_jobs/current_time, successes / len(wait))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we populate our results in a table which we will use to plot the data for later analysis. It is worth noting that based on our prevous results, the database and CPU service time should have to be reduced to, more or less, 0.945374114587 seconds. We consider this our normal value of operation which we will increase to three times according to the specificaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "result = []\n",
    "\n",
    "for value in lambdas:\n",
    "    result.append(vary_interarrival_time(value))\n",
    "\n",
    "df['Interarrival time'] = lambdas\n",
    "\n",
    "column_values = []\n",
    "for value in result:\n",
    "    column_values.append(value[0])\n",
    "    \n",
    "df['N'] = column_values\n",
    "\n",
    "column_values = []\n",
    "for value in result:\n",
    "    column_values.append(value[1])\n",
    "\n",
    "df['U'] = column_values\n",
    "\n",
    "column_values = []\n",
    "for value in result:\n",
    "    column_values.append(value[2])\n",
    "\n",
    "df['R'] = column_values\n",
    "\n",
    "column_values = []\n",
    "for value in lambdas:\n",
    "    column_values.append(value ** -1)\n",
    "\n",
    "df['X'] = column_values\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 2.1</center>\n",
    "\n",
    "The table shows that we are able to sustain with our optimal CPU and DB processors interarrival times between 0.187385 and 0.110227 without falling into queueing necesities. This range also gives residence times below 5 in average and good throughputs between 5 and 9 jobs per second. Above these values, the cost for memory increases exponentially, so it is not feasible to spend in this area. We will plot the previous table below for better visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(221)\n",
    "\n",
    "    plt.scatter(df['X'], df['N'], c=\"Orange\")\n",
    "    plt.ylabel('N')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,10])\n",
    "    axes.set_ylim([0,200])\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.scatter(df['X'], df['U'], c=\"Orange\")\n",
    "    plt.ylabel('U')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,11])\n",
    "    axes.set_ylim([0,1])\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.scatter(df['X'], df['R'], c=\"Orange\")\n",
    "    plt.ylabel('R')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,11])\n",
    "    axes.set_ylim([0,100])\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.scatter(df['X'], df['Interarrival time'], c=\"Orange\")\n",
    "    plt.ylabel('Interarrival time')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,11])\n",
    "    axes.set_ylim([0,0.5])\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore as the arrival rate decreases a clear change in NUR is seen. Utilization increases linearly and at 10 jobs per second is very close to one. Similarly, the residence time increases exponentially once 10 jobs per second is reached but remains fairly constant below this threshold.\n",
    "\n",
    "The number of jobs in the system follows a similar pattern but has a higher steep than the residence time. As long as we don't go above a throughput of 10 jobs per second, the system will be mainly stable and won't have an exponential behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for value in result:\n",
    "    print value[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly we could not achieve these optimal values with a confidence of 95% but that of 50%. The max value gave us 8 on some instances, and the min 3 in others, so we consider this a good result. We belive that the improvement in CPU and DB time, is feasible and could be afforded by the institution, interarrival times also remain feasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part C) Modeling\n",
    "\n",
    "We wish to determine to which queueing model our simulation adheres to best. To do this we divided our analysis into two parts, the first will prove if our simulation correlates to what the Operational Laws dictate as the expected behavior of the system. The second one will use different modeling formulas. The ones we selected to try out were:\n",
    "\n",
    "* Markovian Chains\n",
    "    * M/M/1\n",
    "    * M/M/k\n",
    "    \n",
    "For each one of these models, we will find NURX and compare them.\n",
    "\n",
    "## Operational Laws\n",
    "\n",
    "Based on the project specifications, we have several values to consider. These values are:\n",
    "\n",
    "<center>$\\lambda = 5.3366 \\frac{requests}{second}$</center>\n",
    "<center>$serviceConstants = 0.39$</center>\n",
    "\n",
    "Based on our previous analysis, we determined that our optimal CPU and DB services times should be reduced to 0.94 seconds per request. These are the values used in our simulation, so these are the ones that will be used here.\n",
    "\n",
    "We will start by finding the residence time $R$ which is equal to $S$.\n",
    "\n",
    "<center>$E[S] = serviceConstants + E[t_{cpu}] \\cdot E[g + 1] + E[t_{db}] \\cdot E[g]$</center>\n",
    "\n",
    "Therefore we pass to find the expected value for g which follows a geometric distribution. Our simulation, took into account at least one access to the Database, so it was relatively easy to model as seen in the course since it maps directly to an exponential distribution. It is also worth noting that the summatory is truncated at the value of 4, because that is the constraint of the system based on its specification.\n",
    "\n",
    "<center>$E[g] = \\sum_{k=1}^{4} k  \\cdot P[k] + \\sum_{k=5}^{\\infty} 4 \\cdot P[k]$</center>\n",
    "\n",
    "To simpify such procedure we solve for the first term manually according to the following possible probability values in the summatory. The full process of derivation to get the concrete value is demonstrated below:\n",
    "\n",
    "1. $P(1)$\n",
    "2. $P(2)$\n",
    "3. $P(3)$\n",
    "4. $P(4)$\n",
    "\n",
    "where $P(X)$ being  geometric is equal to $p \\cdot (1-p)^{X-1}$. \n",
    "\n",
    "$firstTerm = \\sum_{k=1}^{4} k\\cdot P[k]$\n",
    "\n",
    "$fristTerm = \\sum_{k=1}^{4} k\\cdot p(1-p)^k$\n",
    "\n",
    "$firstTerm = [1\\cdot P(1) + 2\\cdot P(2) + 3\\cdot P(3) + 4\\cdot P(4)]$\n",
    "\n",
    "Evaluating each P(X) with its corresponding value, we obtain:\n",
    "\n",
    "$firstTerm = 0.5 + 0.5 + 0.375 + .25= 1.625$\n",
    "\n",
    "We use the summatory substitutions to get a closed formula for the second term:\n",
    "\n",
    "$secondTerm = \\sum_{k=5}^{\\infty} 4 \\cdot P[k]$\n",
    "\n",
    "$secondTerm = \\sum_{k=5}^{\\infty} 4 \\cdot p(1-p)^k$\n",
    "\n",
    "$secondTerm = 4p\\cdot\\sum_{k=5}^{\\infty} a^k$\n",
    "\n",
    "$secondTerm = 4p\\cdot\\sum_{j=0}^{\\infty} a^{j+5}$\n",
    "\n",
    "$secondTerm = 4pa^5(\\frac{1}{1 - a})$\n",
    "\n",
    "$secondTerm = 4p(1-p)^5(\\frac{1}{1 - (1-p)})$\n",
    "\n",
    "$secondTerm = 4(1-p)^5$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$E[g] = 3.0625 + 4(1-p)^5 = 1.625 + 0.125 = 1.75$\n",
    "\n",
    "With the value of $E[g]$, we used on of the statistical moments law to calculate $E[g+1]$ which is simply their sum. Therefore,\n",
    "\n",
    "$E[g+1] = E[g] + 1 = 1.75 + 1 = 2.75$\n",
    "\n",
    "With the previous values calculated, we can get the expected service time of the system, which is:\n",
    "\n",
    "$E[S] = 0.39 + 0.94 \\cdot 2.75 + 0.94 \\cdot 1.75 = 4.62$</center>\n",
    "\n",
    "We proceed to find NURX from here:\n",
    "\n",
    "$R = S = 4.62 seconds$\n",
    "\n",
    "$X = \\lambda = 5.3366 \\frac{requests}{second}$\n",
    "\n",
    "$U = \\frac{S \\cdot X}{P} = \\frac{4.62 \\cdot 5.3366}{48} = 0.5136$\n",
    "\n",
    "$N = R \\cdot X = 4.62 \\cdot 5.3366 = 24.65 = 25$\n",
    "\n",
    "Which give very close results to those given by our simulation in normal conditions (See table 2.1, first row to make the comparison). This proves that, based on our assumptions, our simulation is given the correct results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Queuing Theory Modeling\n",
    "\n",
    "Being this an M/M/k model, we wish to compare it to other queueing models seen in the course. Explicitly, we wish to see how well the system behaves if it was implemented in such models. To make such comparison easier to grasp and analyze, we will find NURX($L \\rho W \\lambda$) for each one and do the comparison from there.\n",
    "\n",
    "### M/M/1 Queue model\n",
    "\n",
    "First, we wish to try out the Markovian queue if the system had a single server. The equations used to find NURX are the ones seen in the course and therefore won't be demonstrated here.\n",
    "\n",
    "Since the arrival rate $\\lambda$ is greater than the service rate $\\mu$, the system would start growing the job queue indefinitely, therefore it cannot be modeled as an M/M/1 queue. As prove of it, if we try to get the utilization of the system $\\rho$, it is greater than one and a system's utilization can never be greater or equal to one.\n",
    "\n",
    "The formulas that would have been used are:\n",
    "\n",
    "<center>$\\rho = \\frac{\\lambda}{\\mu}$</center>\n",
    "\n",
    "<center>$L = \\frac{\\rho}{1 - \\rho}$</center>\n",
    "\n",
    "<center>$W = \\frac{L}{\\lambda}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M/M/1/M Queue model\n",
    "\n",
    "Even though we know that the system is not stable when only one processor is present, we wish to model it using a limited queue size of 30 requests and only one processor. Even though the original system has the property of having an unlimited queue, we belive it will be interesting to see how the system behaves under such model and conditions.\n",
    "\n",
    "Since this model was also seen in the course, the equations to find NURX or $L \\rho W \\lambda$ won't be derived.\n",
    "\n",
    "<center>$\\rho = \\frac{\\lambda}{\\mu} = \\frac{5.3366}{0.216} = 24.65$</center>\n",
    "\n",
    "<center>$P_0 = \\frac{1 - \\rho}{1-\\rho^{M+1}} = \\frac{-23.65}{-1.4 \\times 10^{43}} \\approx 0$</center>\n",
    "\n",
    "<center>$U = 1 - P_0 \\approx 1 - 0 \\approx 1$</center>\n",
    "\n",
    "<center>$L = \\frac{\\rho}{1 - \\rho} - \\frac{(M + 1)\\rho^{M+1}}{1-\\rho^{M+1}} = \n",
    "\\frac{24.65}{1 - 24.65} - \\frac{(31)24.65^{31}}{1-24.65^{31}} = -1.042 + 31 = 29.95 \\quad requests$</center>\n",
    "\n",
    "<center>$\\lambda_{efective} = \\mu \\cdot (1 - P_0) = 0.216 * (1 - 0) \\approx 0.216$</center>\n",
    "\n",
    "<center>$W = \\frac{L}{\\lambda_{efective}} = \\frac{29.95}{0.216} = 138.65 \\quad seconds$</center>\n",
    "\n",
    "So, the results show that each job (that is not rejected) will have a residence time of 138.65 seconds, that the queue will always be full and that utilization will be close very close to 1. A clear outcome from a system that is not stable and has an arrival rate which is greater than its service rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M/M/c\n",
    "\n",
    "Based on the contents of the book titled <i>Queuing Formulas</i> found at this [link](http://web.mst.edu/~gosavia/queuing_formulas.pdf) and the GitHub article found [here](http://robharrop.github.io/maths/performance/2016/03/07/multi-server-queues.html), we can find the probability of the system being empty:\n",
    "\n",
    "<center>$P_0 = \\frac{1}{\\sum_{m=0}^{c-1} \\frac{\\rho^m}{m!} + (\\frac{\\rho^c}{c!})(\\frac{1}{1-\\frac{\\rho}{c}})}$</center>\n",
    "\n",
    "Evaluating the previous equations with our real values and being c, our number of servers, 48 we get:\n",
    "\n",
    "<center>$P_0 = \\frac{1}{\\sum_{m=0}^{47}\\frac{24.65^m}{m!}+(\\frac{24.65^{48}}{48!})(\\frac{1}{1-\\frac{24.65}{48}})} \n",
    "= 1.96 \\times 10^{-11}$</center>\n",
    "\n",
    "which is logical, since the arrival rate is by far greater than the service rate, so it will almost always be busy. Now let's find the wait time for a job that enters the system ($W$):\n",
    "\n",
    "<center>$W_q = \\frac{P_c}{\\mu(N-A)}$</center>\n",
    "\n",
    "To find $P_c$ we require Erlangâs C Formula  which value is calculated via:\n",
    "\n",
    "<center>$C(c, \\rho) = \\frac{\\frac{\\rho^c}{c!}\\frac{c}{c-\\rho}}{\\sum_{m=0}^{c-1}\\frac{\\rho^m}{m!} + \\frac{\\rho^c c}{c!(1-\\rho)}}$</center>\n",
    "\n",
    "where $c$ is the number of servers. Making some substitutions and a bit of algebra (explained in more detail [here](https://dspace.vsb.cz/bitstream/handle/10084/84489/AEEE-2011-9-1-7-chromy.pdf?sequence=1)) we find that $C(c, \\rho)$ can be transformed to find $P_n$ which is obtained via:\n",
    "\n",
    "<center>$P_c(N, \\eta) = \\frac{\\frac{(N\\eta)^N}{N!(1-\\eta)}}{\\sum^{N-1}_{i=0}\\frac{(N\\eta)^i}{i!}+\\frac{(N\\eta)^N}{N!(1-\\eta)}}$</center>\n",
    "\n",
    "where $N$ is the number of servers (48) and $\\eta$ is the load or utilization of the system. The utilization can be obtained using the following formula:\n",
    "\n",
    "<center>$\\eta = \\frac{\\lambda}{N\\mu} = \\frac{5.3366}{48*0.216} = 0.5147$</center>\n",
    "\n",
    "We can now start getting concrete values:\n",
    "\n",
    "<center>$P_c(48, 0.5147) = \\frac{\\frac{(48(0.5147))^{48}}{48!(1-0.5147)}}{\\sum^{47}_{i=0}\\frac{(48(0.5147))^i}{i!}+\\frac{(48(0.5147))^{48}}{48!(1-0.5147)}} = 2.2137 \\times 10^{-5}$</center>\n",
    "\n",
    "<center>$W_q = \\frac{2.2137 \\times 10^{-5}}{0.216(48-24.65)} = 4.3892 \\times 10^{-6} \\quad seconds$</center>\n",
    "\n",
    "<center>$W = \\frac{1}{0.216} + 4.3892 \\times 10^{-6} = 4.63 \\quad seconds$</center>\n",
    "\n",
    "which makes sense since no queue is being used because according to the operational laws, all jobs are processed by a server at a given time. Lets calculate the last value $L$ and see if it behaves like our simulation:\n",
    "\n",
    "<center>$L = \\lambda W = 5.3366(4.63) = 24.7 \\quad jobs$</center>\n",
    "\n",
    "So, as expected it behaves very similarly to our real model in all aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M/G/k\n",
    "\n",
    "We decided to move away from the Markovian models and look at more general ones, we tried out first the M/G/k queue. This model uses the M/M/c model to derive the expected residence time $W$ via Kingman's law of congestion.\n",
    "\n",
    "We must consider that this is just an approximation, so for this model del total waiting time $W$ is:\n",
    "\n",
    "<center>$W^{M/G/k} \\approx W^{M/M/c}\\frac{C^2_\\lambda+C^2_s}{2}$</center>\n",
    "\n",
    "it is worth noting that $C^{2}_{X}$ is the squared coefficient of a variation of a positive random variable, and that $S$ is the service time of the system. Given that $\\mu$ and $\\lambda$ are exponentially distribuited, their values are:\n",
    "\n",
    "<center>$C^{2}_{\\lambda} = \\frac{Var[\\lambda]}{E[\\lambda]^2} = \\frac{\\frac{1}{\\lambda^2}}{(\\frac{1}{\\lambda})^2} = 1$</center>\n",
    "\n",
    "<center>$C^{2}_{\\mu} = \\frac{Var[\\mu]}{E[\\mu]^2} = \\frac{\\frac{1}{\\mu^2}}{(\\frac{1}{\\mu})^2} = 1$</center>\n",
    "\n",
    "<center>$W^{M/G/k} \\approx 4.63 \\frac{1 + 1}{2} \\approx 4.63 \\quad seconds$</center>\n",
    "\n",
    "Therefore, when the arrival and service rates are exponentially distribuited, the values for the wait time between the M/G/k and M/M/c models are practically the same. As a result, $\\rho$ and $L$ will be as well, similar, to the M/M/c values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results comparison\n",
    " The models that were most similar to our results in the simulation were M/M/c and M/G/k, while the other two models discussed were not similar at all. In a way, these results are obvious, since the other two models consider only one processor, and our system had 48. Having said that, here is a graph comparing the values gotten for each model and the simulation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, ax = plt.subplots()\n",
    "    n_values = (26.260786, 29.95, 24.7,24.7)\n",
    "    ind = np.arange(4)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars\n",
    "    rects1 = ax.bar(ind, n_values, width, color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_title('Jobs in system')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(('Sim', 'M/M/1/M', 'M/M/c', 'M/G/k'))\n",
    "    ax.set_ylabel('Jobs')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    n_values = (0.547085, 1, .5147,.5147)\n",
    "    ind = np.arange(4)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars\n",
    "    rects1 = ax.bar(ind, n_values, width, color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_title('Utilization')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(('Sim', 'M/M/1/M', 'M/M/c', 'M/G/k'))\n",
    "    plt.show()\n",
    "    \n",
    "    f, ax = plt.subplots()\n",
    "    n_values = (4.8775, 138.65, 4.63,4.63)\n",
    "    ind = np.arange(4)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars\n",
    "    rects1 = ax.bar(ind, n_values, width, color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_title('Residence time')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(('Sim', 'M/M/1/M', 'M/M/c', 'M/G/k'))\n",
    "    ax.set_ylabel('Seconds')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, both the M/M/c and M/G/k model have very similar results to the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "Analysis and simulation have a big impact in the industry and the world. These let us save money in complex cases by letting us understand how something is related to something else and predict how a process will behave. In this project, we had to analyze which component or components were influencing the most a SAP system, specifically regarding response time so we could bring a proposal of how improve it. We found that CPU and DB service times were the most influential. Therefore, to achieve a lower response time we had to invest in CPU processors and DB disks. As previously mentioned, this make companies save money because once you know the root of the problem you donât have to spend money in other components that wonât impact as much in the response time. Thus, the less electronic components we consume the less we damage the environment.\n",
    "\n",
    "E-waste are the circuits and electronic components that we no longer use. At the end of their lifecycle they generally end in the trash. E-waste affects heavily the environment and lately, researchers have not only linked it to environment degradation, but also to adverse effects on human health.\n",
    "\n",
    "Computers are the biggest e-waste source. They compound around the 74% of the number of electronics wasted and Mexico has a significant impact in the generation of e-waste in the world. As we can see they wasted 1.138 millions of tons in the 2012, almost the same number as Russia.\n",
    "\n",
    "![g1](g1.png)\n",
    "\n",
    "Nonetheless, with the pass of the time, it is more often practice to recycle in companies. This can be seen in the chart shown below:\n",
    "\n",
    "![g2](g2.png)\n",
    "\n",
    "We as consumers must be careful with how much we consume. If we take care of the environment we implicitly act in an ethical manner. Currently we are college students, but in some years, we will have our own families. As a result, we have a responsibility to teach them what affects other humans and the world so they can think more ethically before acting. By teaching them ethics and environmental cares we are surely going to make a great impact in the future.\n",
    "\n",
    "E-waste is a real thing, and we must fight it.\n",
    "\n",
    "# References\n",
    "\n",
    "1. IOP (Institute of Physics). 2011. E-waste pollutionâ threat to human health. [source](http://www.iop.org/news/11/may/page_51103.html)\n",
    "\n",
    "2. Livescience. 2013. World's E-Waste to Grow 33% by 2017, Says Global Report. [source](http://www.livescience.com/41967-world-e-waste-to-grow-33-percent-2017.html)\n",
    "\n",
    "3. Electronics TakeBack Coalition. E-Waste In Landfills. [source](http://www.electronicstakeback.com/designed-for-the-dump/e-waste-in-landfills/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
