{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Computational Infrastructure Simulation Project\n",
    "## Cuantitative Methods, Spring 2017\n",
    "## Tec de Monterrey, campus Monterrey\n",
    "\n",
    "Marco A. Peyrot (A00815262)\n",
    "\n",
    "Oliver D. Mendoza (A00513632)\n",
    "\n",
    "Juan Carlos GuzmÃ¡n (A01175826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the libraries required to anaylze the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm, exponweib, lognorm\n",
    "\n",
    "# dynamic plotting information\n",
    "py.plotly.tools.set_credentials_file(username='mpeyrotc', api_key='pNScWhvJN9woL3frF4Vd')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A) Statistical Analysis\n",
    "\n",
    "## Principal Components Selection (Cause-Effect)\n",
    "\n",
    "Starting from the data provided by the course staff about the performance measures for the servers we transferred the data into python through the Pandas library. We take care of cleaning up the data by eliminating null rows and\n",
    "and transforming the time column to a more friendly numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open excel file\n",
    "R3P = pd.ExcelFile(\"R3P Q2 Mayo (ADV).xlsx\")\n",
    "# Open a specific spreadsheet\n",
    "R3P_1 = R3P.parse(\"R3P Q2 Mayo\")\n",
    "\n",
    "R3P_1 = R3P_1.ix[4:,3:31]\n",
    "\n",
    "R3P_1.columns=['Weekday', 'Time', 'Act. WPs', 'Dia.WPs', 'RFC WPs', 'CPU Usr',\n",
    "            'CPU Sys', 'CPU Idle', 'Paging in', 'Paging out', 'Free Mem.', \n",
    "            'EM alloc.', 'EM attach.', 'Em global', 'Heap Memor', 'Private Modes',\n",
    "            'Paging Mem', 'Roll Mem', 'Logins', 'Sessions', '# Pasos Dialogo',\n",
    "            'Resp. Time (Total)', 'CPU (Total)', 'CPU (Prom)', 'BD (Total)', \n",
    "            'BD (Prom)', 'Response Time (Prom)', 'Label']\n",
    "\n",
    "# Clean and reformat data\n",
    "R3P_1.dropna(how=\"all\", inplace=True)\n",
    "R3P_1['Time'] = R3P_1['Time'].astype(str).apply(lambda x: str(x[0]) + str(x[1]))\n",
    "\n",
    "X = R3P_1.ix[:,0:-1]\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once we have loaded the data, we proceed to do PCA (Principal component analysis). We get the contribution that each component provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the probability contribution of all eigen vectors gives 1.\n",
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "    \n",
    "print('Everything ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make cumulative line to visualiza better their contributions\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    plt.bar(range(27), var_exp, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.step(range(27), cum_var_exp, where='mid',\n",
    "             label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis\n",
    "\n",
    "With the results provided by the previous analysis we determined that the first 3 components contribute the most to the actual values. We use biplots to determine correlations between the variables we have. In order to get the PCA analysis for the bibplots we used XLSTAT and loaded the results for F1 and F2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PCAs = pd.ExcelFile(\"PCAs.xlsx\")\n",
    "F1F2 = PCAs.parse(\"F1F2\")\n",
    "\n",
    "F1F2_X = F1F2.ix[104:130,1:28]\n",
    "\n",
    "columns = []\n",
    "for i in xrange(0,27):\n",
    "    columns.append(\"F\" + str(i + 1))\n",
    "\n",
    "F1F2_X.columns=columns\n",
    "\n",
    "F1F2_y = F1F2.ix[104:130,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components F1 and F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    plt.scatter(F1F2_X.T.values[0], F1F2_X.T.values[1], c=\"Orange\")\n",
    "    plt.ylabel('F2 (13.40%)')\n",
    "    plt.xlabel('F1 (35.83%)')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1,1])\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    discriminators = [26, 17, 11, 20, 13, 18, 19, 16, 5]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=len(discriminators), desat=.5)\n",
    "    \n",
    "    for x,y,closest,label in zip(F1F2_X.T.values[0], F1F2_X.T.values[1], range(0,27), F1F2_y.T.values[0]):\n",
    "        plt.plot([0,x], [0, y], '-', linewidth=0.3, c=\"Green\")\n",
    "        if closest not in  discriminators:\n",
    "            plt.annotate(label, xy=(x, y), xytext=(x, y + 0.01))\n",
    "\n",
    "    legend = []\n",
    "    for index, color in zip(discriminators, colors.as_hex()):\n",
    "        plt.scatter(F1F2_X.T.values[0][index], F1F2_X.T.values[1][index], c=color)\n",
    "        legend.append(mpatches.Patch(color=color, label=F1F2_y.T.values[0][index]))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handles=legend)\n",
    "    \n",
    "    plt.plot([-1, 1], [0, 0], '--', linewidth=1, c=\"Black\")\n",
    "    plt.plot([0, 0], [-1, 1], '--', linewidth=1, c=\"Black\")\n",
    "    circle = plt.Circle((0, 0), 1, color='Black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components F1 and F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    \n",
    "    plt.scatter(F1F2_X.T.values[0], F1F2_X.T.values[2], c=\"Orange\")\n",
    "    plt.ylabel('F3 (6.62%)')\n",
    "    plt.xlabel('F1 (35.83%)')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1,1])\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    discriminators = [9, 3, 26, 17, 11, 18, 13, 19, 16]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=len(discriminators), desat=.5)\n",
    "    \n",
    "    for x,y,closest,label in zip(F1F2_X.T.values[0], F1F2_X.T.values[2], range(0,27), F1F2_y.T.values[0]):\n",
    "        plt.plot([0,x], [0, y], '-', linewidth=0.3, c=\"Green\")\n",
    "        if closest not in  discriminators:\n",
    "            plt.annotate(label, xy=(x, y), xytext=(x, y + 0.01))\n",
    "            \n",
    "    legend = []\n",
    "    for index, color in zip(discriminators, colors.as_hex()):\n",
    "        plt.scatter(F1F2_X.T.values[0][index], F1F2_X.T.values[2][index], c=color)\n",
    "        legend.append(mpatches.Patch(color=color, label=F1F2_y.T.values[0][index]))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handles=legend)\n",
    "    \n",
    "    plt.plot([-1, 1], [0, 0], '--', linewidth=1, c=\"Black\")\n",
    "    plt.plot([0, 0], [-1, 1], '--', linewidth=1, c=\"Black\")\n",
    "    circle = plt.Circle((0, 0), 1, color='Black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components F2 and F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    plt.scatter(F1F2_X.T.values[1], F1F2_X.T.values[2], c=\"Orange\")\n",
    "    plt.ylabel('F3 (6.62%)')\n",
    "    plt.xlabel('F2 (13.40%)')\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1,1])\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    discriminators = [11, 19, 18, 17, 16, 13, 9]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=len(discriminators), desat=.5)\n",
    "    \n",
    "    for x,y,closest,label in zip(F1F2_X.T.values[1], F1F2_X.T.values[2], range(0,27), F1F2_y.T.values[0]):\n",
    "        plt.plot([0,x], [0, y], '-', linewidth=0.3, c=\"Green\")\n",
    "        if closest not in  discriminators:\n",
    "            plt.annotate(label, xy=(x, y), xytext=(x, y + 0.01))\n",
    "            \n",
    "    legend = []\n",
    "    for index, color in zip(discriminators, colors.as_hex()):\n",
    "        plt.scatter(F1F2_X.T.values[1][index], F1F2_X.T.values[2][index], c=color)\n",
    "        legend.append(mpatches.Patch(color=color, label=F1F2_y.T.values[0][index]))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handles=legend)\n",
    "    \n",
    "    plt.plot([-1, 1], [0, 0], '--', linewidth=1, c=\"Black\")\n",
    "    plt.plot([0, 0], [-1, 1], '--', linewidth=1, c=\"Black\")\n",
    "    circle = plt.Circle((0, 0), 1, color='Black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical variables selection and dimension reduction\n",
    "\n",
    "Based on the obtained biplots we determined that the following variables are correlated to response time (average) our objective. According to the principal components F1 and F2, relevant variables are:\n",
    "\n",
    "1. BD (Prom)\n",
    "2. CPU(Prom)\n",
    "3. Private modes\n",
    "4. RFC WPs\n",
    "5. Time\n",
    "\n",
    "In the same way, using components F1 and F3 the following variables are also related to response time (average):\n",
    "\n",
    "6. CPU Idle\n",
    "7. CPU Usr\n",
    "\n",
    "Next we show the behavior graphs for each one of these variables in relation to response time. The Orange graphs represent the ones with a visible relation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    plt.subplot(4, 3, 1)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"BD (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('DB (Average) [ms]')\n",
    "    \n",
    "    plt.subplot(4, 3, 2)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    \n",
    "    plt.subplot(4, 3, 3)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,9])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"Private Modes\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('Private Modes [Number]')\n",
    "    \n",
    "    plt.subplot(4, 3, 4)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,40])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"RFC WPs\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('RFC WPs [Number]')\n",
    "    \n",
    "    plt.subplot(4, 3, 5)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,23])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"Time\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('Time [hours]')\n",
    "    \n",
    "    plt.subplot(4, 3, 6)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,100])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU Idle\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU Idle [%]')\n",
    "    \n",
    "    plt.subplot(4, 3, 7)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU Usr\"].T, c=\"Blue\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU Usr [%]')\n",
    "    \n",
    "    response_time = R3P_1[\"Response Time (Prom)\"].apply(math.log10)\n",
    "    cpu_prom = R3P_1[\"CPU (Prom)\"].apply(math.log10)\n",
    "    \n",
    "    plt.subplot(4, 3, 8)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,5])\n",
    "    axes.set_ylim([0,5])\n",
    "    plt.scatter(response_time.T, cpu_prom.T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one was done in a log scale to produce a more precise relation between them. These graphs clearly show a relation between DB (Prom) y CPU(Prom) as potential variables to improve the system performance. We propose mathematical functions which are representative of the system behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    x = R3P_1.loc[:,\"Response Time (Prom)\"][:, np.newaxis]\n",
    "    y = R3P_1.loc[:,\"BD (Prom)\"]\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"BD (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('DB (Average) [ms]')\n",
    "    plt.plot(x, regr.predict(x), color='blue', linewidth=1)\n",
    "    \n",
    "    plt.text(21500, 33000, r'Equation: $y={coef}x+{intercept}$'.format(coef=\"{0:.2f}\".format(regr.coef_[0]), \n",
    "            intercept=\"{0:.2f}\".format(regr.intercept_)), fontsize=12, style='italic', \n",
    "             bbox={'facecolor':'gray', 'alpha':0.1, 'pad':2})\n",
    "\n",
    "    x = R3P_1.loc[:,\"Response Time (Prom)\"][:, np.newaxis]\n",
    "    y = R3P_1.loc[:,\"CPU (Prom)\"]\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,100000])\n",
    "    axes.set_ylim([0,35000])\n",
    "    plt.scatter(R3P_1[\"Response Time (Prom)\"].T, R3P_1[\"CPU (Prom)\"].T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    plt.plot(x, regr.predict(x), color='blue', linewidth=1)\n",
    "    plt.text(21300, 33000, r'Equation: $y={coef}x+{intercept}$'.format(coef=\"{0:.2f}\".format(regr.coef_[0]), \n",
    "                                                                       intercept=\"{0:.2f}\".format(regr.intercept_)),\n",
    "             fontsize=12, style='italic', bbox={'facecolor':'gray', 'alpha':0.1, 'pad':2})\n",
    "    plt.title(\"Response Time vs. CPU (Average)\")\n",
    "    \n",
    "    x = response_time[:, np.newaxis]\n",
    "    y = cpu_prom\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,5])\n",
    "    axes.set_ylim([0,5])\n",
    "    plt.scatter(response_time.T, cpu_prom.T, c=\"Orange\")\n",
    "    plt.xlabel('Response Time (Average) [ms]')\n",
    "    plt.ylabel('CPU (Average) [ms]')\n",
    "    plt.plot(x, regr.predict(x), color='blue', linewidth=1)\n",
    "    plt.text(1.5, 4.7, r'Equation: $y={coef}x+{intercept}$'.format(coef=\"{0:.2f}\".format(regr.coef_[0]), \n",
    "                                                                       intercept=\"{0:.2f}\".format(regr.intercept_)),\n",
    "             fontsize=12, style='italic', bbox={'facecolor':'gray', 'alpha':0.1, 'pad':2})\n",
    "    plt.title(\"Response Time vs. CPU (Average) in log scale\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly for this data the outliers drastically affect the fitting of a general equation. However this is not the case for the CPU average. However, to do a more thorough analysis, we fitted several different trendlines to the data, and the best results yielded were the following:\n",
    "\n",
    "![CPU](CPU trendline.png)\n",
    "![BD](DB trendline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistic analysis\n",
    "\n",
    "Because we found not one, but two critical variables (BD and CPU), we will analyze both, to see which one is more reliable. We will measure its mean, varianza, standard deviation, symetry and kurtosis for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"CPU (Prom):\\n\")\n",
    "print \"Mean:\\t\\t\\t\", R3P_1[\"CPU (Prom)\"].mean()\n",
    "print \"Variance:\\t\\t\", R3P_1[\"CPU (Prom)\"].var()\n",
    "print \"Standard deviation:\\t\", R3P_1[\"CPU (Prom)\"].std()\n",
    "print \"Symetry:\\t\\t\", R3P_1[\"CPU (Prom)\"].skew()\n",
    "print \"Kurtosis:\\t\\t\", R3P_1[\"CPU (Prom)\"].kurtosis()\n",
    "print\n",
    "print(\"BD (Prom):\\n\")\n",
    "print \"Mean:\\t\\t\\t\", R3P_1[\"BD (Prom)\"].mean()\n",
    "print \"Variance:\\t\\t\", R3P_1[\"BD (Prom)\"].var()\n",
    "print \"Standard deviation:\\t\", R3P_1[\"BD (Prom)\"].std()\n",
    "print \"Symetry:\\t\\t\", R3P_1[\"BD (Prom)\"].skew()\n",
    "print \"Kurtosis:\\t\\t\", R3P_1[\"BD (Prom)\"].kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be easily seen that both variables have a heavy tail, since kurtosis is greater than 3. You can see the histogram of the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as p\n",
    "\n",
    "data=R3P_1[\"CPU (Prom)\"]\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"CPU (Prom)\")\n",
    "\n",
    "data=R3P_1[\"BD (Prom)\"]\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"BD (Prom)\")\n",
    "\n",
    "p.legend(loc='upper right')\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be observed that both variables have a similar shape to that of an exponential distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Determining the normality of the data\n",
    "Below, a comparison of both variables that have been discused previously against the exponential distribution will be shown. Originally, our intention was to compare it to the normal distribution, but since it was observed that they had similar behavior to exponential distributions, it was decided to compare against that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "from scipy.stats import norm\n",
    "\n",
    "expectedValues = pd.Series(range(1 ,R3P_1[\"CPU (Prom)\"].count() + 1)).apply(lambda x: (x - 0.5) / R3P_1[\"CPU (Prom)\"].count()).apply(lambda x: expon.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"CPU (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='CPU (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for exponential quartile.'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison against exponential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expectedValues = pd.Series(range(1 ,R3P_1[\"BD (Prom)\"].count() + 1)).apply(lambda x: (x - 0.5) / R3P_1[\"BD (Prom)\"].count()).apply(lambda x: expon.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"BD (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='BD (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for exponential quartile.'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison against exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be observed that both plots have a shape that could, in some way, describe a straight line, so it can be said that, in fact,they have a distribution similar to an exponential. However, its important to note that the variable BD forms something similar to an inverted 'S', which tells us that it has a heavy tail distribution. If we wanted to comparte the variables with the normal distribution, this is what we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "from scipy.stats import norm\n",
    "\n",
    "expectedValues = pd.Series(range(1 ,R3P_1[\"CPU (Prom)\"].count() + 1))\n",
    "expectedValues = expectedValues.apply(lambda x: (x - 0.5) / R3P_1[\"CPU (Prom)\"].count())\n",
    "expectedValues = expectedValues.apply(lambda x: norm.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"CPU (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='CPU (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for normal quartile'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison with normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expectedValues = pd.Series(range(1 ,R3P_1[\"BD (Prom)\"].count() + 1))\n",
    "expectedValues = expectedValues.apply(lambda x: (x - 0.5) / R3P_1[\"BD (Prom)\"].count())\n",
    "expectedValues = expectedValues.apply(lambda x: norm.ppf(x))\n",
    "sortedValues = pd.Series(R3P_1[\"BD (Prom)\"].sort_values()).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a trace\n",
    "trace = Scatter(\n",
    "    x = expectedValues,\n",
    "    y = sortedValues,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title='BD (Prom)',\n",
    "    xaxis=dict(\n",
    "        title='Expected value for normal quartile'\n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Obtained value'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Comparison with normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As it can be seen, instead of looking like straight lines, they seem curved, which tells us that if we compared the with normal distributions, they would be asymetrical, which suggests that the data is not \"normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main variables Analysis\n",
    "\n",
    "The CPU (Average) and Database (average) where the main vaiables that affect response time (seen above). Thankfully we have the traces for that data for a significant period of time. We performed, as last time, PCA analysis over the execution traces for both BD and CPU and proceeded to do the analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## CPU ####################\n",
    "# Obtiene el dataframe de excel\n",
    "CPU_File = pd.ExcelFile(\"CPU Prom Trace.xlsx\")\n",
    "CPU_FactorScores = CPU_File.parse(\"Factor Scores\")\n",
    "\n",
    "# Tabla de Factor Scores\n",
    "Factor_Scores = CPU_FactorScores.ix[:8195,:]\n",
    "\n",
    "# Set X\n",
    "X = Factor_Scores.loc[:,'F1'].values\n",
    "X = np.array(X)\n",
    "\n",
    "CPU_EigenValues = CPU_File.parse(\"Eigen values\")\n",
    "LambdaCPU = CPU_EigenValues.iloc[0]['F1']\n",
    "\n",
    "# Medias y desviaciones estandar Originales\n",
    "MediaOriginalCPU = 2323.994\n",
    "desvEstandarCPU = 552.579\n",
    "\n",
    "# Obtiene las Y\n",
    "Y_CPU = (X * desvEstandarCPU) / math.sqrt(LambdaCPU)\n",
    "\n",
    "for x in range(0, np.size(Y_CPU)):\n",
    "    Y_CPU[x] = Y_CPU[x] + MediaOriginalCPU\n",
    "\n",
    "# Saca la media\n",
    "MediaCPU = sum(Y_CPU)/np.size(Y_CPU)\n",
    "\n",
    "# Elimina los negativos\n",
    "Y_CPU = Y_CPU[Y_CPU >= 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## BD ###################\n",
    "# Obtiene el dataframe de excel\n",
    "BD_File = pd.ExcelFile(\"DB Prom Trace.xlsx\")\n",
    "BD_FactorScores = BD_File.parse(\"Factor Scores\")\n",
    "\n",
    "# Tabla de Factor Scores\n",
    "Factor_Scores = BD_FactorScores.ix[:8195,:]\n",
    "\n",
    "# Set X\n",
    "X = Factor_Scores.loc[:,'F1'].values\n",
    "X = np.array(X)\n",
    "\n",
    "BD_EigenValues = BD_File.parse(\"Eigen values\")\n",
    "LambdaBD = BD_EigenValues.iloc[0]['F1']\n",
    "\n",
    "# Medias y desviaciones estandar Originales\n",
    "MediaOriginalBD = 5038.738\n",
    "desvEstandarBD = 1771.288\n",
    "\n",
    "# Obtiene las Y\n",
    "Y_BD = (X * desvEstandarBD) / math.sqrt(LambdaBD)\n",
    "\n",
    "for x in range(0, np.size(Y_BD)):\n",
    "    Y_BD[x] = Y_BD[x] + MediaOriginalBD\n",
    "\n",
    "# Saca la media\n",
    "MediaBD = sum(Y_BD)/np.size(Y_BD)\n",
    "\n",
    "# Elimina los negativos\n",
    "Y_BD = Y_BD[Y_BD >= 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the CT (Characteristic trace) for CPU and BD in Y_CPU and Y_BD respectively, we can make an statistical analysis of them. This was done the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as p\n",
    "\n",
    "data=Y_CPU\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"Y CPU\")\n",
    "\n",
    "data=Y_BD\n",
    "y,binEdges=np.histogram(data,bins=100)\n",
    "bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "p.plot(bincenters,y,'-', label=\"Y BD\")\n",
    "\n",
    "p.legend(loc='upper right')\n",
    "\n",
    "p.show()\n",
    "\n",
    "Y_CPU2 = pd.Series(Y_CPU)\n",
    "\n",
    "print(\"Y CPU:\\n\")\n",
    "print \"Mean:\\t\\t\\t\", Y_CPU2.mean()\n",
    "print \"Variance:\\t\\t\", Y_CPU2.var()\n",
    "print \"Standard Deviation:\\t\", Y_CPU2.std()\n",
    "print \"Symmetry:\\t\\t\", Y_CPU2.skew()\n",
    "print \"Kurtosis:\\t\\t\", Y_CPU2.kurtosis()\n",
    "\n",
    "Y_BD2 = pd.Series(Y_BD)\n",
    "\n",
    "print\n",
    "print(\"Y BD:\\n\")\n",
    "print \"Mean:\\t\\t\\t\", Y_BD2.mean()\n",
    "print \"Variance:\\t\\t\", Y_BD2.var()\n",
    "print \"Standard Deviation:\\t\", Y_BD2.std()\n",
    "print \"Symmetry:\\t\\t\", Y_BD2.skew()\n",
    "print \"Kurtosis:\\t\\t\", Y_BD2.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can conclude that, since kurtosis is less than 3 in both cases, they are light tailed, and we can see that they are both failry symetrical; not exactly, but enough. Another interesting fact is that while the standard deviation is roughly 23% of its mean in CPU and almost 35% in the case of DB. This shows us that the CPU variable is more consistent the than DB, which could be a relevant piece of indormation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probability distribution adjustment\n",
    "\n",
    "Due to the dificulty and lack of experience in using probability distributions in python, we decided to use XLSTAT again to fit the CPU Y and BD Y vectors.\n",
    "\n",
    "In this section we put the values and graph we got for the following distributions:\n",
    "\n",
    "* Exponencial\n",
    "* Gamma\n",
    "* Weibull 2\n",
    "* Weibull 3\n",
    "* Lognormal\n",
    "* Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass CPU Y and BD Y to csv files to be read by excel\n",
    "Y_CPU.tofile('Y_CPU.csv',sep=',',format='%10.5f')\n",
    "Y_BD.tofile('Y_BD.csv',sep=',',format='%10.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For CPU\n",
    "## Exponential distribution\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|2324.185\t|2324.934|\n",
    "|Variance\t|304725.697\t|304823.833|\n",
    "|Skewness (Pearson)|0.710|\t0.475|\n",
    "|Kurtosis (Pearson)\t|1.613\t|0.338|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.028\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Gamma 2 fit distribution](gamma2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353\t|5036.048|\n",
    "|Variance\t|3135127.217\t|3133071.032|\n",
    "|Skewness (Pearson)|\t0.726|\t0.703|\n",
    "|Kurtosis (Pearson)|\t0.979|\t0.741|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.008\t\n",
    "p-value:\t0.594\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Gamma 2 fit distribution](gamma2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weibull 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|2324.185|\t2310.376|\n",
    "|Variance|\t304725.697|\t379195.850|\n",
    "|Skewness (Pearson)|\t0.710|\t-0.132|\n",
    "|Kurtosis (Pearson)|\t1.613|\t-0.226|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "D: 0.069\t\n",
    "p-value: < 0.0001\t\n",
    "alpha: 0.05\n",
    "\n",
    "![Weibull 2 fit distribution](weibull2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weibull 3\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "| Mean\t|2324.185|\t2324.185|\n",
    "| Variance|\t304725.697|\t295057.687|\n",
    "| Skewness (Pearson)|\t0.710|\t-0.082|\n",
    "| Kurtosis (Pearson)|\t1.613|\t-0.255|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.059\t\n",
    "p-value: < 0.0001\t\n",
    "alpha:\t0.05\t\n",
    "\n",
    "![Weibull 3 fit distribution](weibull3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lognormal\n",
    "\n",
    "| Statistic |\tData|\tParameters|\n",
    "| ------ |\n",
    "|Mean|\t2324.185|\t2325.697|\n",
    "|Variance|\t304725.697|\t320582.598|\n",
    "|Skewness (Pearson)\t|0.710 |\t0.745|\n",
    "|Kurtosis (Pearson)\t|1.613 |\t1.002|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.036\n",
    "\n",
    "p-value: < 0.0001\n",
    "\n",
    "alpha: 0.05\n",
    "\n",
    "![Lognormal fit distribution](lognormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pareto\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|2324.185\t|573728884701.262|\n",
    "|Variance|\t304725.697\t|86292.727|\n",
    "|Skewness (Pearson)\t|0.710|\t2.000|\n",
    "|Kurtosis (Pearson)\t|1.613|\t6.000|\n",
    "\n",
    "Kolmogorov-Smirnov test:\t\n",
    "D:\t0.000\t\n",
    "p-value:\t1.000\t\n",
    "alpha:\t0.05\n",
    "\n",
    "This is an interesting case because the distribution is way off. So no line appears.\n",
    "\n",
    "![Pareto fit distribution](pareto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLSTAT provided us we a very useful value for such fittings, the Kolmogorov-Smirnov test. After some investigation we discovered that this value is used to decide if a sample comes from a population with a specific distribution. This is true for continuous distributions, which we have!\n",
    "\n",
    "For CPU the 3 best fittings where the Exponential, the Gamma 2 and the lognormal with d values 0.028, 0.008, and 0.036 respectively. We made the QQ plots for further analysis for those distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon, lognorm, gamma, weibull_min\n",
    "import statsmodels.api as sm\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_CPU, expon, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Exponential QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_CPU, gamma, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Gamma 2 QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_CPU, lognorm, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Log-normal QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,2])\n",
    "    axes.set_ylim([0,2])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is important for the reader to note that the Pareto distribution gives the D with the lowest value. However it does not correlate visually and we belive that this comes from en error in the tools used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Database\n",
    "## Log normal\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353|\t5057.947|\n",
    "|Variance\t|3135127.217|\t3687505.544|\n",
    "|Skewness (Pearson)\t|0.726|\t1.194|\n",
    "|Kurtosis (Pearson)\t|0.979|\t2.636|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.025\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Log normal fit distribution](lognormaldb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibull 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353\t|5014.704|\n",
    "|Variance|\t3135127.217\t|3349855.894|\n",
    "|Skewness (Pearson)|\t0.726|\t0.173|\n",
    "|Kurtosis (Pearson)\t|0.979|\t-0.269|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.044\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Weibull 2 fit distribution](weibull2db.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibull 3\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean|\t5039.353|\t5039.353|\n",
    "|Variance|\t3135127.217\t|2435772.411|\n",
    "|Skewness (Pearson)|\t0.726\t|0.074|\n",
    "|Kurtosis (Pearson)|\t0.979\t|-0.289|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D:\t0.054\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Weibull 3 fit distribution](weibull3db.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353\t|579254965061.702|\n",
    "|Variance\t|3135127.217\t|156064.120|\n",
    "|Skewness (Pearson)|\t0.726|\t2.000|\n",
    "|Kurtosis (Pearson)\t|0.979\t|6.000|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D: 0.000\t\n",
    "p-value: \t1.000\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Pareto fit distribution](paretodb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5117.023\t|5117.023|\n",
    "|Variance\t|3219173.378|\t0.000|\n",
    "|Skewness (Pearson)\t|0.756\t|2.000|\n",
    "|Kurtosis (Pearson)\t|1.019|\t6.000|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D:\t0.353\t\n",
    "p-value:\t< 0.0001\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Exponential fit distribution](exponentialdb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma 2\n",
    "\n",
    "|Statistic|\tData|\tParameters|\n",
    "| ----|\n",
    "|Mean\t|5039.353|\t5036.048|\n",
    "|Variance|\t3135127.217\t|3133071.032|\n",
    "|Skewness (Pearson)|\t0.726|\t0.703|\n",
    "|Kurtosis (Pearson)\t|0.979|\t0.741|\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "D:\t0.008\t\n",
    "p-value:\t0.594\t\n",
    "alpha:\t0.05\n",
    "\n",
    "![Gamma 2 fit distribution](gamma2db.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Similarly, we can see that for the database the lognormal, weibull 2 y gamma 2 distributions offer the best fit according to the Kolmogorov-Smirnov test. The D values that correspond to each distribution are: 0.025, 0.044, 0.008.\n",
    "\n",
    "Again we plot for these distributions with QQPlots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_BD, lognorm, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Lognormal QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,10000])\n",
    "    axes.set_ylim([0,10000])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We found an outlier that greatly affects the distribution. Making this distribution infeasible for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_BD, weibull_min, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Weibull 2 QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,10000])\n",
    "    axes.set_ylim([0,10000])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again, the outlliers in the database trace make this distribution not suited for our use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    probplot = sm.ProbPlot(Y_BD, gamma, fit=True)\n",
    "    fig = probplot.qqplot(line=\"45\")\n",
    "    \n",
    "    plt.title(\"Gamma 2 QQPlot\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These  results gave us confidence that our system behaves like a gamma probability distribution for both CPU and DB. Needless to say, the Pareto distribution failed returned dubious results. We conclude that we should treat these variables as gamma distribuited and base our future analysis in part B under the assumption that this is their behavior.\n",
    "\n",
    "# Part B) Simulation\n",
    "\n",
    "## Queue system\n",
    "\n",
    "For the simulation we are taking into account that the system has a sigle queue with a message feedback loop. The system is considered to have unlimited memory.\n",
    "\n",
    "The simulation will make use of the following M/M/1 program and the feedback loop will be simulated by using the formula shown below.\n",
    "\n",
    "<center>$interarrivalTime = Random(1,5)X_{CPU} + Random(1,5)BD + additionalData$</center>\n",
    "\n",
    "where additional data is:\n",
    "\n",
    "* Wait Time: 30 ms\n",
    "* Roll-In Time: 30 ms\n",
    "* Load Time: 300 ms\n",
    "* Processing Time: 1200-1300 ms\n",
    "* Database Time: 1200-1300 ms\n",
    "* Roll-Out Time: 30 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "\n",
    "class Processor:\n",
    "    \"\"\"\n",
    "    finish_time: the time in which the processor will finish handling its current job.\n",
    "    is_free: determines if the processor is free to take on a new job.\n",
    "    \"\"\"\n",
    "    finish_time = 0\n",
    "    is_free = True\n",
    "    \n",
    "    def __init__(self, finish_time, is_free):\n",
    "        self.finish_time = finish_time\n",
    "        self.is_free = is_free\n",
    "        \n",
    "def next_interarrival_time(X):\n",
    "    result = (random.randint(1, 5) * X) + (random.randint(1, 5) * 1.25)\n",
    "    return result + 0.03 + 0.03 + 0.3 + 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_free_processor(processors):\n",
    "    \"\"\"\n",
    "    gets a random free processor from the available ones\n",
    "\n",
    "    processors: all the processors in the system.\n",
    "    returns: the Processor that was selected and is curently free to take a job.\n",
    "    \"\"\"\n",
    "    free_processors = []\n",
    "    \n",
    "    for processor in processors:\n",
    "        if processor.is_free:\n",
    "            free_processors.append(processor)\n",
    "            \n",
    "    return free_processors[random.randint(0, len(free_processors) - 1)]\n",
    "\n",
    "def at_least_one_free(processors):\n",
    "    \"\"\"\n",
    "    determines if there is at least one free processor\n",
    "    \n",
    "    processors: processors all the processors present in the system.\n",
    "    returns: true if there is at least one free processor and false otherwise.\n",
    "    \"\"\"\n",
    "    for processor in processors:\n",
    "        if processor.is_free:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_candidate_processor(processors, arrival_time):\n",
    "    \"\"\"\n",
    "    finds the processor which will finish first or one of the free processors (if any)\n",
    "    \n",
    "    processors:  the processors present in the system\n",
    "    arrival_time: the next event arrival time\n",
    "    returns: the selected processor to take the new event (arrival or departure)\n",
    "    \"\"\"\n",
    "    result = processors[0]\n",
    "    \n",
    "    for processor in processors:\n",
    "        if processor.finish_time < result.finish_time:\n",
    "            result = processor\n",
    "            \n",
    "    if result.finish_time > arrival_time and at_least_one_free(processors):\n",
    "        return find_free_processor(processors)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global variables for the simulation\n",
    "processors = []\n",
    "num_processors = 1 \n",
    "total_observation_time = 5000 # cambiar\n",
    "current_time = 0\n",
    "arrival_time = 0\n",
    "num_jobs = 0\n",
    "max_jobs = 999999 # cambiar\n",
    "interarrival_time = 20 # cambiar (JUGAR)\n",
    "last_event = 0\n",
    "time_diff = 0\n",
    "S = 0\n",
    "expected_service_time = 2.6 # cambiar\n",
    "busy_time = 0\n",
    "completed_jobs = 0\n",
    "num_rejected_jobs = 0\n",
    "total_jobs = 0\n",
    "\n",
    "# initialize processors to free\n",
    "for i in xrange(0,num_processors):\n",
    "    processors.append(Processor(999999, True))\n",
    "    \n",
    "while current_time < total_observation_time:\n",
    "    processor = find_candidate_processor(processors, arrival_time)\n",
    "    \n",
    "    # it can be free or occupied\n",
    "    if arrival_time <= processor.finish_time:\n",
    "        if num_jobs < max_jobs:\n",
    "            current_time = arrival_time\n",
    "            num_jobs += 1\n",
    "            time_diff = current_time - last_event\n",
    "            S += (num_jobs - 1) * time_diff\n",
    "            last_event = current_time\n",
    "            \n",
    "            arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "            \n",
    "            if num_jobs <= len(processors):\n",
    "                service_time = next_interarrival_time(-expected_service_time * math.log(random.random()))\n",
    "                processor.finish_time = current_time + service_time\n",
    "                busy_time += min(service_time, total_observation_time - current_time)\n",
    "                processor.is_free = False\n",
    "        else:\n",
    "            arrival_time = current_time - (interarrival_time * math.log(random.random()))\n",
    "            num_rejected_jobs += 1\n",
    "    else:\n",
    "        # there has been a departure from the system\n",
    "        current_time = processor.finish_time\n",
    "        num_jobs -= 1\n",
    "        completed_jobs += 1\n",
    "        time_diff = current_time - last_event\n",
    "        S += (num_jobs + 1) * time_diff\n",
    "        last_event = current_time\n",
    "\n",
    "        if num_jobs >= len(processors):\n",
    "            service_time = next_interarrival_time(-expected_service_time * math.log(random.random()))\n",
    "            processor.finish_time = current_time + service_time\n",
    "            busy_time += min(service_time, total_observation_time - current_time)\n",
    "            processor.is_free = False\n",
    "        else:\n",
    "            processor.is_free = True\n",
    "            processor.finish_time = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"N: {0}\".format(S/current_time)\n",
    "print \"U: {0}\".format(busy_time/(current_time * len(processors)))\n",
    "print \"R: {0}\".format(S/completed_jobs)\n",
    "print \"X: {0}\".format(completed_jobs/current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
